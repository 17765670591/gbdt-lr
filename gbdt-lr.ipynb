{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afc42551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "947e235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10836486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics, model_selection\n",
    "\n",
    "X, Y = datasets.make_classification(n_samples=80, n_features=10)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d35c0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.440440</td>\n",
       "      <td>1.228015</td>\n",
       "      <td>0.839140</td>\n",
       "      <td>0.747243</td>\n",
       "      <td>-2.332011</td>\n",
       "      <td>-1.025721</td>\n",
       "      <td>-0.309797</td>\n",
       "      <td>0.411323</td>\n",
       "      <td>-0.049902</td>\n",
       "      <td>-0.962579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.739745</td>\n",
       "      <td>0.333685</td>\n",
       "      <td>-0.197170</td>\n",
       "      <td>-0.295583</td>\n",
       "      <td>-0.610100</td>\n",
       "      <td>0.524435</td>\n",
       "      <td>-1.410988</td>\n",
       "      <td>-0.075167</td>\n",
       "      <td>-0.031441</td>\n",
       "      <td>-0.492415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.581916</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>1.019422</td>\n",
       "      <td>0.486643</td>\n",
       "      <td>-2.495416</td>\n",
       "      <td>-1.328715</td>\n",
       "      <td>1.686936</td>\n",
       "      <td>-0.121203</td>\n",
       "      <td>0.291490</td>\n",
       "      <td>-0.417412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.535876</td>\n",
       "      <td>2.356874</td>\n",
       "      <td>0.963426</td>\n",
       "      <td>1.560413</td>\n",
       "      <td>-1.560941</td>\n",
       "      <td>-1.450889</td>\n",
       "      <td>1.774207</td>\n",
       "      <td>-1.786244</td>\n",
       "      <td>0.493295</td>\n",
       "      <td>-0.008363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.025944</td>\n",
       "      <td>-0.281376</td>\n",
       "      <td>0.228430</td>\n",
       "      <td>0.874686</td>\n",
       "      <td>0.855791</td>\n",
       "      <td>-0.644039</td>\n",
       "      <td>0.680662</td>\n",
       "      <td>-1.684622</td>\n",
       "      <td>-0.197545</td>\n",
       "      <td>-2.050777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-1.400792</td>\n",
       "      <td>1.091927</td>\n",
       "      <td>0.660738</td>\n",
       "      <td>-0.070337</td>\n",
       "      <td>-1.376937</td>\n",
       "      <td>-0.920058</td>\n",
       "      <td>1.396267</td>\n",
       "      <td>-0.694477</td>\n",
       "      <td>-0.661144</td>\n",
       "      <td>0.170899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.639514</td>\n",
       "      <td>1.293128</td>\n",
       "      <td>-0.301240</td>\n",
       "      <td>0.351279</td>\n",
       "      <td>0.628536</td>\n",
       "      <td>0.419279</td>\n",
       "      <td>0.726491</td>\n",
       "      <td>0.062272</td>\n",
       "      <td>-0.532178</td>\n",
       "      <td>-0.386927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.057271</td>\n",
       "      <td>0.962890</td>\n",
       "      <td>-1.243306</td>\n",
       "      <td>-0.234774</td>\n",
       "      <td>1.198860</td>\n",
       "      <td>2.071980</td>\n",
       "      <td>0.355079</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>-1.620087</td>\n",
       "      <td>-0.403837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.275419</td>\n",
       "      <td>0.281543</td>\n",
       "      <td>-0.290058</td>\n",
       "      <td>-1.173632</td>\n",
       "      <td>1.186925</td>\n",
       "      <td>0.261343</td>\n",
       "      <td>0.308281</td>\n",
       "      <td>-0.883204</td>\n",
       "      <td>-0.176527</td>\n",
       "      <td>0.480768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.301162</td>\n",
       "      <td>0.202779</td>\n",
       "      <td>-0.416862</td>\n",
       "      <td>-0.451544</td>\n",
       "      <td>1.236806</td>\n",
       "      <td>0.490380</td>\n",
       "      <td>0.111233</td>\n",
       "      <td>-0.555125</td>\n",
       "      <td>1.250612</td>\n",
       "      <td>0.000843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0  -2.440440  1.228015  0.839140  0.747243 -2.332011 -1.025721 -0.309797   \n",
       "1  -0.739745  0.333685 -0.197170 -0.295583 -0.610100  0.524435 -1.410988   \n",
       "2  -2.581916 -0.010949  1.019422  0.486643 -2.495416 -1.328715  1.686936   \n",
       "3  -1.535876  2.356874  0.963426  1.560413 -1.560941 -1.450889  1.774207   \n",
       "4   1.025944 -0.281376  0.228430  0.874686  0.855791 -0.644039  0.680662   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "75 -1.400792  1.091927  0.660738 -0.070337 -1.376937 -0.920058  1.396267   \n",
       "76  0.639514  1.293128 -0.301240  0.351279  0.628536  0.419279  0.726491   \n",
       "77  1.057271  0.962890 -1.243306 -0.234774  1.198860  2.071980  0.355079   \n",
       "78  1.275419  0.281543 -0.290058 -1.173632  1.186925  0.261343  0.308281   \n",
       "79  1.301162  0.202779 -0.416862 -0.451544  1.236806  0.490380  0.111233   \n",
       "\n",
       "           7         8         9  \n",
       "0   0.411323 -0.049902 -0.962579  \n",
       "1  -0.075167 -0.031441 -0.492415  \n",
       "2  -0.121203  0.291490 -0.417412  \n",
       "3  -1.786244  0.493295 -0.008363  \n",
       "4  -1.684622 -0.197545 -2.050777  \n",
       "..       ...       ...       ...  \n",
       "75 -0.694477 -0.661144  0.170899  \n",
       "76  0.062272 -0.532178 -0.386927  \n",
       "77  0.005200 -1.620087 -0.403837  \n",
       "78 -0.883204 -0.176527  0.480768  \n",
       "79 -0.555125  1.250612  0.000843  \n",
       "\n",
       "[80 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c8b39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e82c68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'num_leaves': 63,\n",
    "\t'num_trees': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f849dff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\ttraining's binary_logloss: 0.684793\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\ttraining's binary_logloss: 0.676263\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\ttraining's binary_logloss: 0.668238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\ttraining's binary_logloss: 0.660034\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\ttraining's binary_logloss: 0.652319\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\ttraining's binary_logloss: 0.644965\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\ttraining's binary_logloss: 0.637749\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\ttraining's binary_logloss: 0.630669\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\ttraining's binary_logloss: 0.62372\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\ttraining's binary_logloss: 0.6169\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\ttraining's binary_logloss: 0.610262\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\ttraining's binary_logloss: 0.604403\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\ttraining's binary_logloss: 0.597992\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\ttraining's binary_logloss: 0.591696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\ttraining's binary_logloss: 0.585512\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\ttraining's binary_logloss: 0.579158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\ttraining's binary_logloss: 0.572918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\ttraining's binary_logloss: 0.566789\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\ttraining's binary_logloss: 0.560769\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\ttraining's binary_logloss: 0.554855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\ttraining's binary_logloss: 0.548852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\ttraining's binary_logloss: 0.542956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\ttraining's binary_logloss: 0.537163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\ttraining's binary_logloss: 0.531472\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\ttraining's binary_logloss: 0.525881\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\ttraining's binary_logloss: 0.520313\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\ttraining's binary_logloss: 0.515303\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\ttraining's binary_logloss: 0.510383\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\ttraining's binary_logloss: 0.505073\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\ttraining's binary_logloss: 0.499855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\ttraining's binary_logloss: 0.495007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\ttraining's binary_logloss: 0.490238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\ttraining's binary_logloss: 0.485983\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\ttraining's binary_logloss: 0.481802\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\ttraining's binary_logloss: 0.477692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\ttraining's binary_logloss: 0.47314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\ttraining's binary_logloss: 0.468662\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\ttraining's binary_logloss: 0.464256\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\ttraining's binary_logloss: 0.459921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\ttraining's binary_logloss: 0.455655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\ttraining's binary_logloss: 0.451547\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\ttraining's binary_logloss: 0.447505\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\ttraining's binary_logloss: 0.443525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\ttraining's binary_logloss: 0.439608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\ttraining's binary_logloss: 0.435752\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\ttraining's binary_logloss: 0.431926\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\ttraining's binary_logloss: 0.428163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\ttraining's binary_logloss: 0.424462\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\ttraining's binary_logloss: 0.42082\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttraining's binary_logloss: 0.417238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\ttraining's binary_logloss: 0.41359\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\ttraining's binary_logloss: 0.409998\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\ttraining's binary_logloss: 0.406462\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\ttraining's binary_logloss: 0.402979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\ttraining's binary_logloss: 0.399551\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\ttraining's binary_logloss: 0.396225\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\ttraining's binary_logloss: 0.392949\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\ttraining's binary_logloss: 0.389722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\ttraining's binary_logloss: 0.386544\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\ttraining's binary_logloss: 0.383414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\ttraining's binary_logloss: 0.380522\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\ttraining's binary_logloss: 0.377676\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\ttraining's binary_logloss: 0.374872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\ttraining's binary_logloss: 0.372112\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\ttraining's binary_logloss: 0.369394\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\ttraining's binary_logloss: 0.367168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\ttraining's binary_logloss: 0.364741\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\ttraining's binary_logloss: 0.36235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\ttraining's binary_logloss: 0.359996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\ttraining's binary_logloss: 0.357891\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\ttraining's binary_logloss: 0.354968\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\ttraining's binary_logloss: 0.352437\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\ttraining's binary_logloss: 0.349597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\ttraining's binary_logloss: 0.346799\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\ttraining's binary_logloss: 0.344044\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\ttraining's binary_logloss: 0.34152\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\ttraining's binary_logloss: 0.339178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\ttraining's binary_logloss: 0.336871\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\ttraining's binary_logloss: 0.334598\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\ttraining's binary_logloss: 0.332209\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\ttraining's binary_logloss: 0.330147\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\ttraining's binary_logloss: 0.327893\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\ttraining's binary_logloss: 0.32567\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\ttraining's binary_logloss: 0.323478\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\ttraining's binary_logloss: 0.321317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\ttraining's binary_logloss: 0.319245\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\ttraining's binary_logloss: 0.317201\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\ttraining's binary_logloss: 0.315186\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\ttraining's binary_logloss: 0.313197\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\ttraining's binary_logloss: 0.311235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\ttraining's binary_logloss: 0.309204\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\ttraining's binary_logloss: 0.307201\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\ttraining's binary_logloss: 0.305225\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\ttraining's binary_logloss: 0.303547\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\ttraining's binary_logloss: 0.301622\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\ttraining's binary_logloss: 0.299829\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\ttraining's binary_logloss: 0.298061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\ttraining's binary_logloss: 0.296316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\ttraining's binary_logloss: 0.294594\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's binary_logloss: 0.293068\n",
      "Save model...\n",
      "Start predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangweiliang/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "# number of leaves,will be used in feature transformation\n",
    "num_leaf = 63\n",
    "\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_train)\n",
    "\n",
    "print('Save model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict and get data on leaves, training data\n",
    "y_pred = gbm.predict(X_train,pred_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a4819c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 100)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9157f5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f98c39bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 6300)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature transformation and write result\n",
    "print('Writing transformed training data')\n",
    "transformed_training_matrix = np.zeros([len(y_pred),len(y_pred[0]) * num_leaf],dtype=np.int64)\n",
    "transformed_training_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6e4c4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,   63,  126,  189,  252,  315,  378,  441,  504,  567,  630,\n",
       "        693,  756,  819,  882,  945, 1008, 1071, 1134, 1197, 1260, 1323,\n",
       "       1386, 1449, 1512, 1575, 1638, 1701, 1764, 1827, 1890, 1953, 2016,\n",
       "       2079, 2142, 2205, 2268, 2331, 2394, 2457, 2520, 2583, 2646, 2709,\n",
       "       2772, 2835, 2898, 2961, 3024, 3087, 3150, 3213, 3276, 3339, 3402,\n",
       "       3465, 3528, 3591, 3654, 3717, 3780, 3843, 3906, 3969, 4032, 4095,\n",
       "       4158, 4221, 4284, 4347, 4410, 4473, 4536, 4599, 4662, 4725, 4788,\n",
       "       4851, 4914, 4977, 5040, 5103, 5166, 5229, 5292, 5355, 5418, 5481,\n",
       "       5544, 5607, 5670, 5733, 5796, 5859, 5922, 5985, 6048, 6111, 6174,\n",
       "       6237])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48e6065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4750e309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,len(y_pred)):\n",
    "\ttemp = np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[i])\n",
    "\ttransformed_training_matrix[i][temp] += 1\n",
    "\n",
    "#for i in range(0,len(y_pred)):\n",
    "#\tfor j in range(0,len(y_pred[i])):\n",
    "#\t\ttransformed_training_matrix[i][j * num_leaf + y_pred[i][j]-1] = 1\n",
    "transformed_training_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37cd57bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate feature importances...\n",
      "Feature importances: [85, 0, 0, 0, 15, 0, 0, 0, 0, 0]\n",
      "Feature importances: [1882.1921005249023, 0.0, 0.0, 0.0, 251.7265911102295, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Logestic Regression Start\n",
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "print('Calculate feature importances...')\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))\n",
    "print('Feature importances:', list(gbm.feature_importance(\"gain\")))\n",
    "\n",
    "\n",
    "# Logestic Regression Start\n",
    "print(\"Logestic Regression Start\")\n",
    "\n",
    "# load or create your dataset\n",
    "print('Load data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f6fc09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([1,0.5,0.1,0.05,0.01,0.005,0.001])\n",
    "for t in range(0,len(c)):\n",
    "\tlm = LogisticRegression(penalty='l2',C=c[t]) # logestic model construction\n",
    "\tlm.fit(transformed_training_matrix,y_train)  # fitting the data\n",
    "\n",
    "\t#y_pred_label = lm.predict(transformed_training_matrix )  # For training data\n",
    "\t#y_pred_label = lm.predict(transformed_testing_matrix)    # For testing data\n",
    "\t#y_pred_est = lm.predict_proba(transformed_training_matrix)   # Give the probabilty on each label\n",
    "\ty_pred_est = lm.predict_proba(transformed_training_matrix)   # Give the probabilty on each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01a855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d4de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c7d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e77bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
